{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Applying_deep_learning_to_NLP",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIGjBVxfs_m"
      },
      "source": [
        "## Deep Learning NLP - Getting Started\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mdHUdLdHTf"
      },
      "source": [
        "### Load the Data\n",
        "Instead of reloading the data, we restore the ones we have stored in the previous notebook. I have stored them already and they are available for download.\n",
        "\n",
        "Note that this tuturial was written on PyTorch version `1.0.1`. If you use the newest version of PyTorch to run the code here you may run into some issues due to compatability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_F9azIpMz2q",
        "outputId": "4d5d95f6-7025-494a-9892-aee38eb2286b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "## takes roughly about 3 minutes\n",
        "!pip install torch==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/92/1ae072a56665e36e81046d5fb8a2f39c7728c25c21df1777486c49b179ae/torch-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (560.0MB)\n",
            "\u001b[K     |████████████████████████████████| 560.1MB 29kB/s \n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "Successfully installed torch-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBLZ0sFdE9G",
        "outputId": "52f72922-ce5f-4b23-eebf-c5e9f4ff6f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# helper functions\n",
        "def convert_to_pickle(item, directory):\n",
        "    pickle.dump(item, open(directory,\"wb\"))\n",
        "\n",
        "\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCo-O0StGPP-"
      },
      "source": [
        "Let's first download our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atCgea7PL5jN",
        "outputId": "b7729e67-1673-4a0e-c397-8509b6832d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/qcyl34jvdc9siw6/test_dataset\n",
        "!wget https://www.dropbox.com/s/ldk80nwz5va1wvz/train_dataset\n",
        "!wget https://www.dropbox.com/s/t4cah3zc9bz6jnv/val_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-14 14:29:53--  https://www.dropbox.com/s/qcyl34jvdc9siw6/test_dataset\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/qcyl34jvdc9siw6/test_dataset [following]\n",
            "--2020-04-14 14:29:53--  https://www.dropbox.com/s/raw/qcyl34jvdc9siw6/test_dataset\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com/cd/0/inline/A10HSAoFyIRoi8lRBThJ8BcZRKcY6DevzS6mGONqKZQDoystpjC9uTIsgJsh_QyiGH20UQprjdGONnHNcKx0OiKX-1BkJElQp-XWDnUnHVw8HDwqk67VTtZrhtCHuVsseMw/file# [following]\n",
            "--2020-04-14 14:29:53--  https://uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com/cd/0/inline/A10HSAoFyIRoi8lRBThJ8BcZRKcY6DevzS6mGONqKZQDoystpjC9uTIsgJsh_QyiGH20UQprjdGONnHNcKx0OiKX-1BkJElQp-XWDnUnHVw8HDwqk67VTtZrhtCHuVsseMw/file\n",
            "Resolving uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com (uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com (uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A10_XfevPgHkvoIzXc5ofAMicdWmV6E-tJVqHmRNiuLQ-vNthc0cJyzyxVTk2zLMlBNUaC-h8cqXK9C-Ifkl0Q3lcpQinZjRURoNUAcK28egKo1xaSMra1FS9WFU40K0DpJdZq2On04QPjwhl1pXmWLUWzzDXWy8TYQWezsiN-EKno6Htg1SuZkjinSOkfU_MtSOX6rGqiVnKHYTM0RLEQcojZWJrUponIOjctrD6i86iYW4padomPSPEaEOUrFR2SE6vykjf4AP4fhwk4_KZVV4lnpysoLiDk8yY61QsLmZnzsOOeB9WV-tllCav1t3xZVgEpspfUnoXNu84rWZ-zj9E8auHJJNrdFOpVlslt52mg/file [following]\n",
            "--2020-04-14 14:29:54--  https://uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com/cd/0/inline2/A10_XfevPgHkvoIzXc5ofAMicdWmV6E-tJVqHmRNiuLQ-vNthc0cJyzyxVTk2zLMlBNUaC-h8cqXK9C-Ifkl0Q3lcpQinZjRURoNUAcK28egKo1xaSMra1FS9WFU40K0DpJdZq2On04QPjwhl1pXmWLUWzzDXWy8TYQWezsiN-EKno6Htg1SuZkjinSOkfU_MtSOX6rGqiVnKHYTM0RLEQcojZWJrUponIOjctrD6i86iYW4padomPSPEaEOUrFR2SE6vykjf4AP4fhwk4_KZVV4lnpysoLiDk8yY61QsLmZnzsOOeB9WV-tllCav1t3xZVgEpspfUnoXNu84rWZ-zj9E8auHJJNrdFOpVlslt52mg/file\n",
            "Reusing existing connection to uc6ea348adcd9027152bd80db9b3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3455150 (3.3M) [application/octet-stream]\n",
            "Saving to: ‘test_dataset’\n",
            "\n",
            "test_dataset        100%[===================>]   3.29M  13.1MB/s    in 0.3s    \n",
            "\n",
            "2020-04-14 14:29:55 (13.1 MB/s) - ‘test_dataset’ saved [3455150/3455150]\n",
            "\n",
            "--2020-04-14 14:29:57--  https://www.dropbox.com/s/ldk80nwz5va1wvz/train_dataset\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ldk80nwz5va1wvz/train_dataset [following]\n",
            "--2020-04-14 14:29:57--  https://www.dropbox.com/s/raw/ldk80nwz5va1wvz/train_dataset\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com/cd/0/inline/A12RESiiD7Rq16RK2xe472cqNBuQvu4t6zELFyzAOSgLEwqikLvv9HngxGmstdStONOKzxdc_W-YaVBQXH4KK4nX7JuLlULx6mi7mX45-WF6pNesYo2bm25JXMlYFxpQXsg/file# [following]\n",
            "--2020-04-14 14:29:58--  https://uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com/cd/0/inline/A12RESiiD7Rq16RK2xe472cqNBuQvu4t6zELFyzAOSgLEwqikLvv9HngxGmstdStONOKzxdc_W-YaVBQXH4KK4nX7JuLlULx6mi7mX45-WF6pNesYo2bm25JXMlYFxpQXsg/file\n",
            "Resolving uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com (uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com (uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A110IJXwPKJ-Sx75VCo2AgBHdEjJWC7kVQPxTfBBOI8Qxjmq5qboRptbO5oUabKFLLz_FOcjviZJl87vysGOsLXrIkVQ3xaJKgKfrIFzbUBEAd4bduA92_wrgCccAMGZqAulEqCI2tVuMTzykxKa4aGS66x60vauO_1PnmHjQdOh51MOhTMFYCaRCXU-WNYiPaV-qvv0xLQckRQRa2vy_1iRRTLLHAS_4inaV2m6epUKyFn2cCP2R-iy-5Hh_8TBHH-9DqdogDC8t9VPzpbpBJm5IONfhWpSvn-lqEfEXG0f9D8Nad0YrlncRWksqWFp5vur62vdM_HAWRtgKkGfn9b2biCKWlMNe0UVfB0DokDV4w/file [following]\n",
            "--2020-04-14 14:29:58--  https://uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com/cd/0/inline2/A110IJXwPKJ-Sx75VCo2AgBHdEjJWC7kVQPxTfBBOI8Qxjmq5qboRptbO5oUabKFLLz_FOcjviZJl87vysGOsLXrIkVQ3xaJKgKfrIFzbUBEAd4bduA92_wrgCccAMGZqAulEqCI2tVuMTzykxKa4aGS66x60vauO_1PnmHjQdOh51MOhTMFYCaRCXU-WNYiPaV-qvv0xLQckRQRa2vy_1iRRTLLHAS_4inaV2m6epUKyFn2cCP2R-iy-5Hh_8TBHH-9DqdogDC8t9VPzpbpBJm5IONfhWpSvn-lqEfEXG0f9D8Nad0YrlncRWksqWFp5vur62vdM_HAWRtgKkGfn9b2biCKWlMNe0UVfB0DokDV4w/file\n",
            "Reusing existing connection to uc7e62b6bec5945c704040bf28b3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27640290 (26M) [application/octet-stream]\n",
            "Saving to: ‘train_dataset’\n",
            "\n",
            "train_dataset       100%[===================>]  26.36M  10.8MB/s    in 2.4s    \n",
            "\n",
            "2020-04-14 14:30:01 (10.8 MB/s) - ‘train_dataset’ saved [27640290/27640290]\n",
            "\n",
            "--2020-04-14 14:30:02--  https://www.dropbox.com/s/t4cah3zc9bz6jnv/val_dataset\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/t4cah3zc9bz6jnv/val_dataset [following]\n",
            "--2020-04-14 14:30:03--  https://www.dropbox.com/s/raw/t4cah3zc9bz6jnv/val_dataset\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com/cd/0/inline/A10YNuODAaxN8_UMYe-74-5L5wb3rpUzSTUua2vQP6rWce3ZMYyywB9muRADITd2zTjBb-GSF8s-jbuEudhK6bixZ5X05rZWH8OiJEnjibjKRUEGPcy838lOzUUVFCHj1Ck/file# [following]\n",
            "--2020-04-14 14:30:03--  https://uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com/cd/0/inline/A10YNuODAaxN8_UMYe-74-5L5wb3rpUzSTUua2vQP6rWce3ZMYyywB9muRADITd2zTjBb-GSF8s-jbuEudhK6bixZ5X05rZWH8OiJEnjibjKRUEGPcy838lOzUUVFCHj1Ck/file\n",
            "Resolving uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com (uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com (uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A10Y7VQccBtexOiM8_TOGQmaHwXF3K7OIcNccd4WC-NQbvM_p6Z9QN4AQokhYNK_I5sENbJNADqx6M82pPxggvvlb_VxZnyQ1Kdm7jJGLlgbVCQ6iMFzhbfu7gZVWkdFKkYmELcyyXBzH9oTP8t1AuuRszN-cnFZgcjjpPrtR_ijQJPN2BxrmEm3XccaPj4un0sx-EMPUAMq2SSmwZxcH66K2fHJBFPjifV5TYmQKUHTxxLilQYxYBoyOkMSiktnhswzBu-w2ualRN2FKBJ-zJQtqH6AE4OzgysRVx9rnlX5hNvGxfc3S4TKa-6wj1NgXSS6rXlXAg8uhpCd3_MgfQv7sSWc4_Aua31J2I_pWYbElA/file [following]\n",
            "--2020-04-14 14:30:04--  https://uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com/cd/0/inline2/A10Y7VQccBtexOiM8_TOGQmaHwXF3K7OIcNccd4WC-NQbvM_p6Z9QN4AQokhYNK_I5sENbJNADqx6M82pPxggvvlb_VxZnyQ1Kdm7jJGLlgbVCQ6iMFzhbfu7gZVWkdFKkYmELcyyXBzH9oTP8t1AuuRszN-cnFZgcjjpPrtR_ijQJPN2BxrmEm3XccaPj4un0sx-EMPUAMq2SSmwZxcH66K2fHJBFPjifV5TYmQKUHTxxLilQYxYBoyOkMSiktnhswzBu-w2ualRN2FKBJ-zJQtqH6AE4OzgysRVx9rnlX5hNvGxfc3S4TKa-6wj1NgXSS6rXlXAg8uhpCd3_MgfQv7sSWc4_Aua31J2I_pWYbElA/file\n",
            "Reusing existing connection to uce36202f17d20779f0dcc0f57a1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3455150 (3.3M) [application/octet-stream]\n",
            "Saving to: ‘val_dataset’\n",
            "\n",
            "val_dataset         100%[===================>]   3.29M  6.23MB/s    in 0.5s    \n",
            "\n",
            "2020-04-14 14:30:04 (6.23 MB/s) - ‘val_dataset’ saved [3455150/3455150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcE-Q3_MRVn"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "We did the data loading in the previous notebook, we just carry the code over here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKt8eof-IqgA"
      },
      "source": [
        "## You need to declare the class again to properly load the data\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x, y, x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "## store the datasets in these variables\n",
        "train_dataset = load_from_pickle(\"train_dataset\")\n",
        "test_dataset = load_from_pickle(\"test_dataset\")\n",
        "val_dataset = load_from_pickle(\"val_dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHq06XGJkV4",
        "outputId": "3c0a9a82-b561-4c31-b849-7bc3b04b7685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset.batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP3VrH2h5Q7C"
      },
      "source": [
        "### Implementing Model\n",
        "\n",
        "After the data has been preprocessed, transformed and prepared it is now time to build the model or the so-called computation graph that will be used to train our classification model. We are going to use a gated recurrent neural network (GRU), which is considered a more efficient version of a basic RNN. The figure below shows a high-level overview of the model details. \n",
        "\n",
        "The model aims to learn representations and project those to the classification task via a fully connected layer followed by a softmax operation that produces values that sum to 1 and can be interpreted as probabilities. Essentially, the outputs of the RNN are mapped to a probability distribution over the predecited output classes. \n",
        "\n",
        "![alt txt](https://github.com/omarsar/nlp_pytorch_tensorflow_notebooks/blob/master/img/gru-model.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWjzFMkLKFKS"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXwdHHeQfugz"
      },
      "source": [
        "class EmoGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        ## layers\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5) # avoid overfitting\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "    \n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.hidden_units)).to(device)\n",
        "    \n",
        "    def forward(self, x, lens, device):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
        "        out = output[-1, :, :] \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return out, self.hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzcFnWB07Ev2"
      },
      "source": [
        "### Model sanity testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqSmsXuoKDM2"
      },
      "source": [
        "# parameters\n",
        "TRAIN_BUFFER_SIZE = 40000 # len(input_tensor_train)\n",
        "VAL_BUFFER_SIZE = 5000 # len(input_tensor_val)\n",
        "TEST_BUFFER_SIZE = 5000 # len(input_tensor_test)\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
        "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE\n",
        "TEST_N_BATCH = TEST_BUFFER_SIZE // BATCH_SIZE\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = 27291 # len(inputs.word2idx)\n",
        "target_size = 6 # num_emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CekF-k7m7F_i"
      },
      "source": [
        "## put batches of same size closer to each other; generally helps the model\n",
        "## read more here: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
        "def sort_batch(X, y, lengths):\n",
        "    \"sort the batch by length\"\n",
        "    \n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) => (seq x batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WziHVOpG7Lcz",
        "outputId": "9d61052f-f94f-4f25-84b4-126c47506b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "## obtain one sample from the data iterator\n",
        "it = iter(train_dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "## sort the batch first to be able to use with pad_packed_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "print(\"Input size: \", xs.size())\n",
        "\n",
        "output, _ = model(xs.to(device), lens, device)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input size:  torch.Size([69, 64])\n",
            "torch.Size([64, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUeoDIDX7Ovk"
      },
      "source": [
        "### Setup Training\n",
        "Now that we have tested the model, it is time to train it. We will define out optimization algorithm, learnin rate, and other necessary information to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeuPVVgw7SZU"
      },
      "source": [
        "## Enabling cuda\n",
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmoGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\n",
        "model.to(device)\n",
        "\n",
        "## loss criterion and optimizer for training\n",
        "criterion = nn.CrossEntropyLoss() # the same as log_softmax + NLLLoss\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "def loss_function(y, prediction):\n",
        "    \"\"\" CrossEntropyLoss expects outputs and class indices as target \"\"\"\n",
        "    ## convert from one-hot encoding to class indices\n",
        "    target = torch.max(y, 1)[1]\n",
        "    loss = criterion(prediction, target) \n",
        "    return loss   \n",
        "    \n",
        "def accuracy(target, logit):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
        "    corrects = (torch.max(logit, 1)[1].data == target).sum()\n",
        "    accuracy = 100.0 * corrects / len(logit)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc3tkiqQ9JzH"
      },
      "source": [
        "### Training Model\n",
        "\n",
        "Now we finally train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujawKs7x9Lc8",
        "outputId": "08a0a126-ed70-4aa9-9664-38762026c244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "## takes ~3 minutes\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    ### Initialize hidden state\n",
        "    # TODO: do initialization here.\n",
        "    total_loss = 0\n",
        "    train_accuracy, val_accuracy = 0, 0\n",
        "    \n",
        "    ### Training\n",
        "    for (batch, (inp, targ, lens)) in enumerate(train_dataset):\n",
        "        loss = 0\n",
        "        predictions, _ = model(inp.permute(1 ,0).to(device), lens, device) # TODO:don't need _   \n",
        "              \n",
        "        loss += loss_function(targ.to(device), predictions)\n",
        "        batch_loss = (loss / int(targ.shape[1]))        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        train_accuracy += batch_accuracy\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Val. Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.cpu().detach().numpy()))\n",
        "            \n",
        "    ### Validating\n",
        "    for (batch, (inp, targ, lens)) in enumerate(val_dataset):        \n",
        "        predictions,_ = model(inp.permute(1, 0).to(device), lens, device)        \n",
        "        batch_accuracy = accuracy(targ.to(device), predictions)\n",
        "        val_accuracy += batch_accuracy\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1, \n",
        "                                                             total_loss / TRAIN_N_BATCH, \n",
        "                                                             train_accuracy / TRAIN_N_BATCH,\n",
        "                                                             val_accuracy / VAL_N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Val. Loss 0.2982\n",
            "Epoch 1 Batch 100 Val. Loss 0.2278\n",
            "Epoch 1 Batch 200 Val. Loss 0.2175\n",
            "Epoch 1 Batch 300 Val. Loss 0.1060\n",
            "Epoch 1 Batch 400 Val. Loss 0.0351\n",
            "Epoch 1 Batch 500 Val. Loss 0.0328\n",
            "Epoch 1 Batch 600 Val. Loss 0.0330\n",
            "Epoch 1 Loss 0.1436 -- Train Acc. 66.0000 -- Val Acc. 90.0000\n",
            "Time taken for 1 epoch 30.53244161605835 sec\n",
            "\n",
            "Epoch 2 Batch 0 Val. Loss 0.0348\n",
            "Epoch 2 Batch 100 Val. Loss 0.0174\n",
            "Epoch 2 Batch 200 Val. Loss 0.0275\n",
            "Epoch 2 Batch 300 Val. Loss 0.0281\n",
            "Epoch 2 Batch 400 Val. Loss 0.0449\n",
            "Epoch 2 Batch 500 Val. Loss 0.0149\n",
            "Epoch 2 Batch 600 Val. Loss 0.0271\n",
            "Epoch 2 Loss 0.0272 -- Train Acc. 92.0000 -- Val Acc. 91.0000\n",
            "Time taken for 1 epoch 30.768707752227783 sec\n",
            "\n",
            "Epoch 3 Batch 0 Val. Loss 0.0091\n",
            "Epoch 3 Batch 100 Val. Loss 0.0153\n",
            "Epoch 3 Batch 200 Val. Loss 0.0308\n",
            "Epoch 3 Batch 300 Val. Loss 0.0387\n",
            "Epoch 3 Batch 400 Val. Loss 0.0369\n",
            "Epoch 3 Batch 500 Val. Loss 0.0239\n",
            "Epoch 3 Batch 600 Val. Loss 0.0358\n",
            "Epoch 3 Loss 0.0202 -- Train Acc. 93.0000 -- Val Acc. 92.0000\n",
            "Time taken for 1 epoch 31.226889610290527 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnAM43gri7LJ"
      },
      "source": [
        "### Stopping the Model\n",
        "\n",
        "How do we know when to stop the model. We can use a technique called `early stopping`, not covered here, but widely used in deep learning, to control the convergence of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7w1P3PX9Qjq"
      },
      "source": [
        "### Store the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvcWh9S9PHQ",
        "outputId": "b9b1fdc3-fa75-4c1e-cea8-bd9810ec9e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.save(model, \"/gdrive/My Drive/pycon2019/emogru\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type EmoGRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cB1oo9_cnN"
      },
      "source": [
        "---\n",
        "\n",
        "### Exercise - Implementing your deep learning model\n",
        "Implement a model similar to the one above. Try to use an LSTM instead of an GRU. Go into the pytorch documentation and research quick ways to improve the model, like adding a `Dropout` [layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html). Also, add additional layers (i.e., make it deeper) to improve the model potential.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4WVcdF_xCI"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nrQaqmceV6V"
      },
      "source": [
        "### References\n",
        "- [Emotion Recognition with PyTorch](https://github.com/omarsar/emotion_recognition_pytorch/blob/master/Deep_Learning_Emotion_Recognition_PyTorch.ipynb)\n",
        "\n",
        "- [Serialization Semantics by PyTorch](https://pytorch.org/docs/master/notes/serialization.html#recommend-saving-models)\n",
        "\n",
        "- [Word embeddings in PyTorch](https://github.com/omarsar/phd_2017/blob/master/pytorch_word_embeddings.ipynb)\n"
      ]
    }
  ]
}